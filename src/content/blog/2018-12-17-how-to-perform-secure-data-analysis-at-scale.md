---
meta_title: Learn How to Perform Secure Data Analysis at Scale | LimeProxies
yoast_keyword: 'Secure Data Analysis '
description: >-
  Data analysis refers to the technique of collecting raw data, analysing it and
  transforming it into information that can be used to reach a specific
  conclusion.
path: /how-to-perform-secure-data-analysis-at-scale/
title: How to Perform Secure Data Analysis at Scale?
author: Rachael Chapman
date: 2018-12-17T06:17:20.000Z
excerpt: >-
  The history of data analysis dates back more than 1000 BC during the Egyptian
  era and is more relevant than ever in 2018. This shows how important data
  analysis is and will continue to be.
thumbnail: /assets/Data-AAnalysis-04.png
altText: hadoop
tags:
  - How to Guides
  - Perform Secure Data Analysis
  - Secure Data Analysis
  - Secure Data Analysis at Scale
featurednail: /assets/Data-AAnalysis-04.png
---
<span style="font-weight: 400;">The history of data analysis dates back more than 1000 BC during the Egyptian era and is more relevant than ever in 2018. This shows how important data analysis is and will continue to be.</span>

**Data analysis** <span style="font-weight: 400;">refers to the technique of collecting raw data, analysing it and transforming it into information that can be used to reach a specific conclusion.</span>

Let&#8217;s see the various stages of data analysis and for this consider an example where a student has to analyse whether private or government schools are better.

<li style="font-weight: 400;">
  <span style="font-weight: 400;">The initial or the first phase would be </span><b>Setting Objective</b><span style="font-weight: 400;">, it means you should know the reason behind collecting and analysing the data. In this case our objective is to prove which schooling system is better.</span>
</li>
<li style="font-weight: 400;">
  <span style="font-weight: 400;">Once the objective is set, now the next phase can be divided into two parts; the first part is to determine </span><b>what data to be analysed</b><span style="font-weight: 400;"> and the next is </span><b>how it has to be done</b><span style="font-weight: 400;">. </span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">When it comes to “</span><i><span style="font-weight: 400;">What data to be Analysed</span></i><span style="font-weight: 400;">”, considering the example, it will be quality of education provided, successful passed, basic knowledge of subjects grade wise, etc</span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">For “</span><i><span style="font-weight: 400;">How data has to be Analysed</span></i><span style="font-weight: 400;">”, parameters like what benchmark to select for checking the quality of education, questions that need to be included to test the basic subject knowledge, etc </span>
</li>
<li style="font-weight: 400;">
  <span style="font-weight: 400;">As the objective and the parameters are set, the next phase is </span><b>collect or gather all the information</b><span style="font-weight: 400;"> that are available relevant to the second phase, it’s not necessary that all the information be useful, but in this phase, the importance is to only collect as much as data as possible. </span>
</li>
<li style="font-weight: 400;">
  <span style="font-weight: 400;">In this phase all the collected data is segregated, all the unnecessary information is discarded and the useful ones are organized. The best way to do this by asking the following questions:</span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">1. Does the collected data provide satisfactory answers to the question about schools?</span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">2. Will the data collected prove useful to defend counter questions?</span>
</li>

<span style="font-weight: 400;">Once all stages are completed, you will have a well organized data that will help you find a solution to the question that we took as an example.</span>

### **How do you improve your data analysis skills on a daily basis?**

<span style="font-weight: 400;">Few basic habits can actually make a big difference. Following the below habits can have a huge impact on how you develop your skills:</span>

* **Gather**<span style="font-weight: 400;">: This is where you need to go through materials related to Data Analytics, there are various tutorials available online, read books, watch videos, etc</span>
* **Documentation**<span style="font-weight: 400;">: Once you gather information, it is important to retain that and what&#8217;s better than making your own notes? They can be just scribblings about the highlight of a particular topic in Data Analytics, this will help you go back to ideas that arose during the tutorial. </span>
* **Art of Application**<span style="font-weight: 400;">: Whatever information you have, it is important to apply it practically, try to come up with scenarios and find a solution to it.</span>

<span style="font-weight: 400;">One of the most important part that an analyst overlooks is the use of </span>**proxies**<span style="font-weight: 400;">.</span>

### **Proxies can help in the following ways:**

<span style="font-weight: 400;">&#8211; Automating the process of extracting the data without worrying about the IP getting blocked</span>

<span style="font-weight: 400;">&#8211; Spoofing location and getting geo-specific data</span>

**Big Data**<span style="font-weight: 400;">, as the name suggests is a collection of large data, this collection can be in the form of structured or unstructured data.</span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">This data can help an organization to get insights that can further help them in taking data driven decisions, thus improving the overall progress of the company. </span>

<span style="font-weight: 400;">Earlier Big Data was often recognized by 3Vs but these have expanded and now can be characterized by 6Vs.</span>

<span style="font-weight: 400;">1. The data collected from different sources which are relevant to the organization can be characterized as a </span>**volume**

<span style="font-weight: 400;">2. Information comes in a </span>**variety** <span style="font-weight: 400;">of formats. They range from organized, conventional databases, also unstructured content, email, video, sound, etc. </span>

<span style="font-weight: 400;">3. The third most important characteristic is the speed or </span>**velocity** <span style="font-weight: 400;">at which the data is collected.</span>

<span style="font-weight: 400;">4. One of the additional characteristics is </span>**veracity**<span style="font-weight: 400;">, this means the level of authenticity of the collected data.</span>

<span style="font-weight: 400;">5. Once the data is collected and structured, the next important characteristic to measure is its </span>**value** <span style="font-weight: 400;">i.e the value that it holds to the organization</span>

**6. Variability** <span style="font-weight: 400;">helps us understand the various ways the collected data can be used.</span>

### **How does Big Data leave an impact?**<span style="font-weight: 400;"><br /> </span>

<span style="font-weight: 400;">The size or the volume of data collected is not important but how the collected information is utilized is what matters. </span><span style="font-weight: 400;"><br /> </span>

<span style="font-weight: 400;">The data that is collected can be analyzed to get information or answers to cost effectiveness, proper time management, data driven decisions and when big data and such analytics are mixed together, you can easily determine:</span>

* <span style="font-weight: 400;">The exact reason for system failure, what are the pressing issues and other effects almost in real time</span>
* <span style="font-weight: 400;">Analyzing the behavior of the potential buyer and thus improving or personalizing the sign up process</span>
* <span style="font-weight: 400;">With such a massive amount of data, it is easy to know the fraud patterns and can be blocked before it gives a massive blow</span>

&nbsp;

### **Who uses Big Data**<span style="font-weight: 400;">:</span><span style="font-weight: 400;"><br /> </span>

<span style="font-weight: 400;">1. </span>**Banking Sector**<span style="font-weight: 400;">: When it comes to banking, there is always a large amount of data that flows in all the time. Banking sector usually uses this information to be to improve security and better the customer experience. But banks have to make use of powerful analytics tool to take real advantage of this data.</span>

<span style="font-weight: 400;">2. </span>**Schooling and overall education**<span style="font-weight: 400;">: Usually the need for big data analysis is played down when it comes to education. But it can turn out to be very crucial steps towards improving the overall education system. Big data analysis can easily show key results related to student’s performance, the average passing percentage and what other improvements can be done in the system.</span>

<span style="font-weight: 400;">3. </span>**Healthcare Industry**<span style="font-weight: 400;">: This industry or rightly put as “service” benefits with their patients’ data, like their previous diagnosis, medication taken or that is being used and any other patient record. Analysing these data can prove crucial for an effective treatment.</span>

<span style="font-weight: 400;">4. </span>**Manufacturing**<span style="font-weight: 400;">: When large volumes of data from the manufacturing industry are analysed, it helps in improving the quality of the product, customer service and helps companies to realize market trends and what a customer wants. </span>

<span style="font-weight: 400;">5. </span>**Administration**<span style="font-weight: 400;">: Administration can also be defined as a large body with a lot of subdivisions and the data generated from each department can be humongous. To provide a better way of life to the people of the region, to understand their issues and to be able to quickly implement </span>

<img class="alignnone wp-image-3044 size-full" src="/assets/Data-AAnalysis-04.png" alt="what is hadoop" width="720" height="361" />

<span style="font-weight: 400;">It is a software that allows the processing of Big data across a lot of computers using a simple program.</span>

### **The problems with Big data and how Hadoop solves it**

**1. Storage**<span style="font-weight: 400;">: Nowadays the size of big data for an <a style="color: #333333; text-decoration: none;" href="https://actionsolar.net/">solar panels actionsolar.net san diego</a> organization grows exponentially and it is not cost effective in putting resources in the high storage server. </span><span style="font-weight: 400;"><br /> </span>_<span style="font-weight: 400;">Resolution</span>_<span style="font-weight: 400;">: Hadoop uses HDFS i.e Hadoop Distributed File System which can store data in different hardware but process the same data parallelly. </span>

**2. High volume of data**<span style="font-weight: 400;">: This creates an issue because the data that is received can be structured, unstructured or semi-structured and especially in different formats</span><span style="font-weight: 400;"><br /> </span>_<span style="font-weight: 400;">Resolution: </span>_<span style="font-weight: 400;">This issue is again resolved by Hadoop Distributed File System as there is no pre-dumpling schema validation, so whenever a new data is out under HDFS, there is no need to define the schema.</span>

**3. Computing power**<span style="font-weight: 400;">: The size of data that usually comes in nowadays is in Terabytes and it will take a great amount of time to process all these data. For example, you have 2 TB of data and a computing power of 1Gbps, so the total time taken to process all these data will be around 34 minutes and this will significantly increase if the volume of data is very high</span><span style="font-weight: 400;"><br /> </span>__

_<span style="font-weight: 400;">Resolution: </span>_<span style="font-weight: 400;">Hadoop uses a cluster of computers or it’s functioning and hence the processing of data is run parallelly and this significantly reduces the computing time. </span>

**Configuring Hadoop**<span style="font-weight: 400;"><br /> </span>

<span style="font-weight: 400;">Let’s dive a bit into the technical aspect of Hadoop configuration. The following are the steps for a single node installation. </span>

#### **Prerequisites**<span style="font-weight: 400;"><br /> </span>

<span style="font-weight: 400;">&#8211; Hadoop needs a working Java 1.5+ (aka Java 5) installation.</span><span style="font-weight: 400;"><br /> </span>

<pre><span style="font-weight: 400;">user@ubuntu:~$  sudo apt-get update #Updating the source list</span></pre>

<span style="font-weight: 400;">or Install it </span>

<pre><span style="font-weight: 400;">user@ubuntu:~$ sudo apt-get install sun-java6-jdk</span></pre>

#### **Adding a dedicated Hadoop system user**

<pre><span style="font-weight: 400;">user@ubuntu:~$ sudo addgroup hadoop_group</span>
<span style="font-weight: 400;">user@ubuntu:~$ sudo adduser --ingroup hadoop_group hdusert</span></pre>

<span style="font-weight: 400;">This will add the user </span>_<span style="font-weight: 400;">hdusert</span>_ <span style="font-weight: 400;">and the group hadoop_group to the local machine. </span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">Add </span>_<span style="font-weight: 400;">hdusert</span>_ <span style="font-weight: 400;">to the sudo group</span>

<pre><span style="font-weight: 400;">user@ubuntu:~$ sudo adduser hdusert sudo</span></pre>

#### **Configuring SSH**

<span style="font-weight: 400;">A key has to be generated for the hduser user</span>

<pre><span style="font-weight: 400;">user@ubuntu:~$ su – hdusert</span>
<span style="font-weight: 400;">hdusert@ubuntu:~$ ssh-keygen -t rsa -P ""</span>

</pre>

<span style="font-weight: 400;"><strong>NOTE</strong>: P “”, here indicates an empty password</span>

<span style="font-weight: 400;">This will create an RSA key pair with an empty password.</span>

<span style="font-weight: 400;">Now SSH access has to be enabled for your local machine with this created key and  is done by the following command.</span>

<pre><span style="font-weight: 400;">hdusert@ubuntu:~$   cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys</span></pre>

#### **Installation**

<span style="font-weight: 400;">The first step is to switch to hduser</span>

<pre><span style="font-weight: 400;">hduser@ubuntu:~$ su - hdusert</span></pre>

<span style="font-weight: 400;">Next step is to download and extract Hadoop 1.2.0. Also, Setup Environment Variables for Hadoop</span><span style="font-weight: 400;"><br /> </span>

<pre><span style="font-weight: 400;">export HADOOP_HOME=/usr/local/hadoop</span>
<span style="font-weight: 400;"># Add Hadoop bin/ directory to PATH</span>
<span style="font-weight: 400;">export PATH= $PATH:$HADOOP_HOME/bin</span></pre>

#### **Configuration**

<span style="font-weight: 400;">Change the file: conf/hadoop-env.sh</span><span style="font-weight: 400;"><br /> </span>

<pre><span style="font-weight: 400;">#export JAVA_HOME=/usr/lib/j2sdk1.5-sun</span></pre>

<span style="font-weight: 400;"><br /> </span><span style="font-weight: 400;">in the following file</span><span style="font-weight: 400;"><br /> </span>

<pre><span style="font-weight: 400;"># export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64  (for 64 bit)</span>
<span style="font-weight: 400;"># export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64  (for 32 bit)</span></pre>

<span style="font-weight: 400;">Create the directory and set the required ownerships and permissions</span>

<pre><span style="font-weight: 400;">hduser@ubuntu:~$ sudo mkdir -p /app/hadoop/tmp</span>

<span style="font-weight: 400;">hduser@ubuntu:~$ sudo chown hduser:hadoop /app/hadoop/tmp</span>

<span style="font-weight: 400;">hduser@ubuntu:~$ sudo chmod 750 /app/hadoop/tmp</span></pre>

<span style="font-weight: 400;">Paste the following between <configuration></span>

<span style="font-weight: 400;">In file conf/core-site.xml</span>

<pre><span style="font-weight: 400;">&lt;property&gt;</span>
<span style="font-weight: 400;">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span>
<span style="font-weight: 400;">    &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;</span>
<span style="font-weight: 400;">    &lt;description&gt;A base for other temporary directories.&lt;/description&gt;</span>
<span style="font-weight: 400;">&lt;/property&gt;</span>

<span style="font-weight: 400;">&lt;property&gt;</span>
<span style="font-weight: 400;">    &lt;name&gt;fs.default.name&lt;/name&gt;</span>
<span style="font-weight: 400;">    &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;</span>
<span style="font-weight: 400;">    &lt;description&gt;The name of the default file system.  A URI whose</span>
<span style="font-weight: 400;">    scheme and authority determine the FileSystem implementation.  The</span>
<span style="font-weight: 400;">    uri's scheme determines the config property (fs.SCHEME.impl) naming</span>
<span style="font-weight: 400;">    the FileSystem implementation class.  The uri's authority is used to</span>
<span style="font-weight: 400;">    determine the host, port, etc. for a filesystem.&lt;/description&gt;</span>
<span style="font-weight: 400;">&lt;/property&gt;</span></pre>

<span style="font-weight: 400;">In file conf/mapred-site.xml</span><span style="font-weight: 400;"><br /> </span>

<pre><span style="font-weight: 400;">&lt;property&gt;</span>
<span style="font-weight: 400;">&lt;name&gt;mapred.job.tracker&lt;/name&gt;</span>
<span style="font-weight: 400;">    &lt;value&gt;localhost:54311&lt;/value&gt;</span>
<span style="font-weight: 400;">    &lt;description&gt;The host and port that the MapReduce job tracker runs</span>
<span style="font-weight: 400;">    at. If "local", then jobs are run in-process as a single map</span>
<span style="font-weight: 400;">    and reduce task.</span>
<span style="font-weight: 400;">    &lt;/description&gt;</span>
<span style="font-weight: 400;">&lt;/property&gt;</span>

</pre>

<span style="font-weight: 400;">In file conf/hdfs-site.xml</span><span style="font-weight: 400;"><br /> </span>

<pre><span style="font-weight: 400;">&lt;property&gt;</span>
<span style="font-weight: 400;">    &lt;name&gt;dfs.replication&lt;/name&gt;</span>
<span style="font-weight: 400;">    &lt;value&gt;1&lt;/value&gt;</span>
<span style="font-weight: 400;">    &lt;description&gt;Default block replication.</span>
<span style="font-weight: 400;">    The actual number of replications can be specified when the file is created.</span>
<span style="font-weight: 400;">    The default is used if replication is not specified in create time.</span>
<span style="font-weight: 400;">    &lt;/description&gt;</span>
<span style="font-weight: 400;">&lt;/property&gt;</span></pre>

<span style="font-weight: 400;">Now Format the HDFS filesystem via the NameNode</span>

<span style="font-weight: 400;">hduser@ubuntu:~$ /usr/local/hadoop/bin/hadoop namenode –format</span>

<span style="font-weight: 400;">Finally, starting the single node cluster</span>

<pre><span style="font-weight: 400;">hduser@ubuntu:~$ sudo chmod -R 777 /usr/local/hadoop</span></pre>

<span style="font-weight: 400;">Run the following command</span>

<pre><span style="font-weight: 400;">hduser@ubuntu:~$ /usr/local/hadoop/bin/start-all.sh</span></pre>

**This will startup a Namenode, Datanode, Jobtracker and a Tasktracker on the machine.**

<pre><span style="font-weight: 400;">hduser@ubuntu:/usr/local/hadoop$ jps</span></pre>

### **Secure big data analysis:**

<span style="font-weight: 400;">As there are tons of data that are downloaded every day, the challenges related to security also increases many folds. Few of them are listed below:</span>

<img class="alignnone wp-image-3046 size-full" src="/assets/Data-AAnalysis-01.png" alt="Data in Cloud" width="720" height="360" />

<span style="font-weight: 400;">Cloud storage has facilitated a way to store a large amount of data but it comes with privacy and security issues. You should be careful whom you select as your cloud provider and get the details of the security setup before purchase</span>

Also Read **<a class="row-title" href="/blog/big-data-examples-in-real-life/" aria-label="“Real Life Examples Of The Application Of Big Data Analytics” (Edit)">Real Life Examples Of The Application Of Big Data Analytics</a>**

<img class="alignnone wp-image-3045 size-full" src="/assets/Data-AAnalysis-02.png" alt="Access Control" width="720" height="360" />

<span style="font-weight: 400;">This is also a very important security feature that is overlooked that involves not only deciding which users get access to the data but also how much access is provided to each user</span>

<img class="alignnone wp-image-3043 size-full" src="/assets/Data-AAnalysis-03.png" alt="Data Protection" width="720" height="361" />

<span style="font-weight: 400;">When data is collected on such a massive scale, there are chances that sensitive information gets mixed up and leaked.</span>

### **Conclusion**<span style="font-weight: 400;">:</span>

<span style="font-weight: 400;">Big data is a boon is so many ways, it can help an organization evaluate their performance, make corrections and optimize their delivery.</span>

<span style="font-weight: 400;">They should also secure the data from a breach and other security vulnerability. When the data is collected there should be real time monitoring and as the information stored runs into Terabytes of data, proper measures should be in place to safeguard it. These are the few steps that can be undertaken for secure Big Data Analysis</span>

<h2 style="text-align: center;">
  <button style="background-color: #9acd32; border-radius: 5%; border: solid 2px #9ACD32;"><a style="color: #eeeeee;" href="https://dashboard.limeproxies.com/?utm_source=blog&utm_content=how%20to%20renew%20ip%20address#/login/signup" target="_blank" rel="noopener noreferrer">GET STARTED FOR FREE</a></button>
</h2>
